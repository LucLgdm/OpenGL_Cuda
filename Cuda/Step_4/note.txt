1. Matrix Transpose Shared

Objectif : comprendre l‚Äôacc√®s coalescent √† la m√©moire globale.
Principe :
	Transpose une matrice carr√©e A en B en utilisant la shared memory pour r√©duire les acc√®s non coalescents.

Difficult√©s √† surmonter :
	Charger les blocs de A dans la shared memory de mani√®re coalescente.
	Utiliser un padding (TILE_SIZE+1) pour √©viter les bank conflicts.
	Synchroniser les threads avant d‚Äô√©crire dans B.

üí° C‚Äôest l‚Äôun des exercices les plus classiques pour comprendre l‚Äôoptimisation m√©moire dans CUDA.

2. Matrix Vector Multiplication (Shared Memory)

Objectif : r√©utiliser les donn√©es de la matrice efficacement.
Principe :
	Multiplie une matrice A (N√óN) par un vecteur x (N√ó1) pour produire y = A¬∑x.

√Ä faire :
	Chaque bloc traite une portion de A et de x.
	Charger les morceaux de x dans la shared memory pour √©viter de les relire √† chaque ligne.
	R√©duire partiellement dans la shared memory avant d‚Äô√©crire le r√©sultat global.

3. Vector Dot Product (Multi-Block Reduction)

Tu as d√©j√† fait la version intra-bloc.
Fais maintenant la version multi-bloc, o√π :
	chaque bloc √©crit sa somme partielle dans un tableau partial[],
	puis un second kernel ou une boucle CPU fait la r√©duction finale.

Variante : fais-le enti√®rement sur GPU avec deux kernels cha√Æn√©s.

4. Element-Wise Matrix Operations

Objectif : exploiter la shared memory comme cache local.
Principe :
	Fais des op√©rations √©l√©ment par √©l√©ment :
	C[i][j] = sin(A[i][j]) + cos(B[i][j]);
mais en stockant des sous-blocs de A et B dans la shared memory avant d‚Äôappliquer la transformation.

Tu apprendras √† utiliser la shared memory non seulement pour le produit mais aussi pour des pipelines de calcul.

5. 2D Convolution (Shared Memory)

Objectif : comprendre le data reuse spatial.
Principe :
	Applique une convolution 2D simple (filtre 3√ó3 ou 5√ó5) sur une image.

√âtapes :
	Chaque bloc charge une tuile de l‚Äôimage + halo dans la shared memory.
	Chaque thread applique le filtre √† son pixel.
	N√©cessite des calculs d‚Äôoffset pr√©cis et des bordures.

üí° C‚Äôest une transition id√©ale vers les kernels plus ‚Äúavanc√©s‚Äù du Step 5 (pipeline, overlapping, etc.).

6. Matrix Multiplication Benchmark

Objectif : profiler les gains m√©moire.
Principe :
	Compare les trois versions de la multiplication de matrices :

	1. naive (acc√®s global uniquement),
	2. shared memory (ton kernel actuel),
	3. version mixte (avec const __restrict__ ou m√©moire constante pour une des matrices).

But : mesurer l‚Äôimpact des acc√®s coalescents et de la shared memory.

7. Reduction with Shared Memory (Generic Version)

Objectif : g√©n√©raliser les r√©ductions.
Principe :
	√âcris un kernel de r√©duction g√©n√©rique :
	template<typename T, typename Op>
	__global__ void reduceShared(const T* in, T* out, int n, Op op);
et passe-lui par exemple op = Add(), op = Max(), op = Mul()...

Tu d√©couvriras ici la pattern classique d‚Äôun reduction kernel moderne, utile pour les op√©rations de sommation,
max pooling, etc.
